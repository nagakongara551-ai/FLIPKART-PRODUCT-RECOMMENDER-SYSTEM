from langchain_groq import ChatGroq
from langchain_classic.chains.history_aware_retriever import create_history_aware_retriever
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from flipkart.config import Config
from langchain_core.vectorstores import VectorStore # Use core base class for typing

class RAGChainBuilder:
    """
    Constructs a complete, conversational Retrieval-Augmented Generation (RAG) pipeline
    using LCEL, the Groq LLM, and a vector store.

    The chain maintains conversation history to rewrite follow-up questions for better retrieval.
    """
    def __init__(self, vector_store: VectorStore, k: int = 3, temperature: float = 0.5):
        """
        Initializes the RAGChainBuilder.

        Args:
            vector_store (VectorStore): The vector store instance containing the documents.
            k (int, optional): The number of top documents to retrieve. Defaults to 3.
            temperature (float, optional): The LLM generation temperature. Defaults to 0.5.
        """
        self.vector_store = vector_store
        self.k = k
        self.model = ChatGroq(model=Config.RAG_MODEL, temperature=temperature)
        
        # NOTE: For production, this MUST be replaced with a persistent store (e.g., Redis, DynamoDB).
        self.history_store = {} 

    def _get_history(self, session_id: str) -> BaseChatMessageHistory:
        """
        Retrieves or creates a new in-memory chat history object for a given session ID.

        Args:
            session_id (str): A unique identifier for the user's conversation session.

        Returns:
            BaseChatMessageHistory: The chat history object for the session.
        """
        if session_id not in self.history_store:
            self.history_store[session_id] = ChatMessageHistory()
        return self.history_store[session_id]
    
    def build_chain(self):
        """
        Constructs and returns the final conversational RAG chain as a Runnable.

        The chain flows as: History -> Query Rephrase -> Retrieval -> Answer Generation.

        Returns:
            RunnableWithMessageHistory: The full, stateful RAG chain, ready for invocation.
        """
        # --- 1. Setup Retriever ---
        retriever = self.vector_store.as_retriever(search_kwargs={"k": self.k})

        # --- 2. Contextualization Prompt (Query Rephrasing) ---
        context_prompt = ChatPromptTemplate.from_messages([
            ("system", 
             "Given the chat history and the latest user question, rewrite it as a standalone search query. "
             "Do NOT answer the question; only output the standalone query."
            ),
            MessagesPlaceholder(variable_name="chat_history"), 
            ("human", "{input}")  
        ])

        # --- 3. QA Prompt (Final Answer Generation) ---
        # NOTE: We remove redundant history/input placeholders since the 'input' 
        # to this chain is already the complete, standalone question generated by 
        # the history_aware_retriever.
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", 
             """You are an e-commerce bot answering product-related queries using reviews and titles.
             Stick strictly to the provided CONTEXT. Be concise and helpful.
             
            CONTEXT:
            {context}

            QUESTION: {input}"""
            ),
        ])

        # --- 4. Chain Construction ---
        
        # Part A: Intelligent Retriever that uses history
        history_aware_retriever = create_history_aware_retriever(
            self.model, retriever, context_prompt
        )

        # Part B: Chain that combines retrieved documents and generates the answer
        question_answer_chain = create_stuff_documents_chain(
            self.model, qa_prompt
        )

        # Part C: Combines the intelligent retriever with the QA chain
        rag_chain = create_retrieval_chain(
            history_aware_retriever, question_answer_chain
        )

        # --- 5. Final Runnable with History ---
        return RunnableWithMessageHistory(
            rag_chain,
            self._get_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            # The output of the rag_chain is a dict, and 'answer' is the key holding the response.
            output_messages_key="answer" 
        )